{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNNs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYOqZZ-40cEu",
        "colab_type": "text"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P59NYU98GCb9",
        "colab": {}
      },
      "source": [
        "!pip3 -qq install torch\n",
        "!pip3 -qq install bokeh==0.13.0\n",
        "!pip3 -qq install gensim==3.6.0\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn==0.20.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8sVtGHmA9aBM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TiA2dGmgF1rW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "30a43558-cdaa-4141-abce-6276f3df5f5e"
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QstS4NO0L97c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "e472f11a-fa7f-4522-e361-e414fe63247f"
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xTai8Ta0lgwL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "ed5dffc0-2dfa-4c56-cc84-74299465fa5b"
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pCjwwDs6Zq9x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "658bbe84-c39d-4f07-e110-062060cdc6fe"
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'DET', 'ADJ', 'NUM', 'X', '.', 'VERB', 'ADP', 'ADV', 'PRON', 'NOUN', 'CONJ', 'PRT'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "URC1B2nvPGFt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "52cc112b-f4ec-43db-ff73-8adb634cadee"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAEvCAYAAAAemFY+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkyRJ6pAhTZIkqUOGNEmSpA79f4lfhLWgjT+PAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rWmSToIaeAo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cdcad9a3-7c1b-4c0b-d67a-b6e038ea33ca"
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vjz_Rk0bbMyH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d31a840f-0eca-4f30-df6e-994537a00c83"
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XCuxEBVbOY_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "62f549b1-eeb7-44ac-85b8-287b88eb1ff6"
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtRbz1SwgEqc",
        "colab": {}
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhsTKZalfih6",
        "colab": {}
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4XsRII5kW5x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fc423e5f-815c-491b-ac9f-04c0e85d86bf"
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVEHju54d68T",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        # self.word_emb_dim = word_emb_dim\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.lstm_layers_count = lstm_layers_count\n",
        "        self.embed = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self.fc = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embed(inputs)\n",
        "        before_tags, _ = self.lstm(embeds)\n",
        "        tags = self.fc(before_tags)\n",
        "        softmax_tags = F.log_softmax(tags, dim=1)\n",
        "        return softmax_tags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZmPO1I9gE8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_accuracy(logits, y_batch):\n",
        "  preds = torch.argmax(logits, 2)\n",
        "  mask = (y_batch != 0).float()\n",
        "  truth = ((preds == y_batch).float() * mask).sum()\n",
        "  return truth , mask.sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbrxsZ2mehWB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "14dc8b6d-c805-4c7a-b969-9eab974ea2ca"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n",
        "calc_accuracy(logits, y_batch)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(7.), tensor(92.))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxZ3sjUBfp6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_loss(criterion, logits, y_batch):\n",
        "  return criterion(logits.transpose(2, 1), y_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GMUyUm1hgpe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "75f2492e-740f-4b1e-cb4b-765fa78b733f"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "calc_loss(criterion, logits, y_batch)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.5726, grad_fn=<NllLoss2DBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FprPQ0gllo7b",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "\n",
        "                loss = calc_loss(criterion, logits, y_batch)\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                cur_correct_count, cur_sum_count = calc_accuracy(logits, y_batch)\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pqfbeh1ltEYa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "afaa5590-f7a5-4749-ed44-3f6275a01c37"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.72445, Accuracy = 77.99%: 100%|██████████| 572/572 [00:04<00:00, 120.83it/s]\n",
            "[1 / 50]   Val: Loss = 0.40332, Accuracy = 86.11%: 100%|██████████| 13/13 [00:00<00:00, 94.10it/s]\n",
            "[2 / 50] Train: Loss = 0.29212, Accuracy = 90.32%: 100%|██████████| 572/572 [00:04<00:00, 122.05it/s]\n",
            "[2 / 50]   Val: Loss = 0.28510, Accuracy = 90.18%: 100%|██████████| 13/13 [00:00<00:00, 94.69it/s]\n",
            "[3 / 50] Train: Loss = 0.20335, Accuracy = 93.27%: 100%|██████████| 572/572 [00:04<00:00, 121.19it/s]\n",
            "[3 / 50]   Val: Loss = 0.23482, Accuracy = 91.86%: 100%|██████████| 13/13 [00:00<00:00, 90.71it/s]\n",
            "[4 / 50] Train: Loss = 0.15571, Accuracy = 94.84%: 100%|██████████| 572/572 [00:04<00:00, 126.00it/s]\n",
            "[4 / 50]   Val: Loss = 0.21490, Accuracy = 92.55%: 100%|██████████| 13/13 [00:00<00:00, 96.76it/s]\n",
            "[5 / 50] Train: Loss = 0.12378, Accuracy = 95.83%: 100%|██████████| 572/572 [00:04<00:00, 122.79it/s]\n",
            "[5 / 50]   Val: Loss = 0.19444, Accuracy = 93.14%: 100%|██████████| 13/13 [00:00<00:00, 94.45it/s]\n",
            "[6 / 50] Train: Loss = 0.10216, Accuracy = 96.57%: 100%|██████████| 572/572 [00:04<00:00, 122.59it/s]\n",
            "[6 / 50]   Val: Loss = 0.19154, Accuracy = 93.27%: 100%|██████████| 13/13 [00:00<00:00, 94.09it/s]\n",
            "[7 / 50] Train: Loss = 0.08431, Accuracy = 97.15%: 100%|██████████| 572/572 [00:04<00:00, 123.82it/s]\n",
            "[7 / 50]   Val: Loss = 0.18787, Accuracy = 93.43%: 100%|██████████| 13/13 [00:00<00:00, 95.26it/s]\n",
            "[8 / 50] Train: Loss = 0.07046, Accuracy = 97.61%: 100%|██████████| 572/572 [00:04<00:00, 125.85it/s]\n",
            "[8 / 50]   Val: Loss = 0.19241, Accuracy = 93.45%: 100%|██████████| 13/13 [00:00<00:00, 96.37it/s]\n",
            "[9 / 50] Train: Loss = 0.05935, Accuracy = 97.98%: 100%|██████████| 572/572 [00:04<00:00, 126.69it/s]\n",
            "[9 / 50]   Val: Loss = 0.19531, Accuracy = 93.51%: 100%|██████████| 13/13 [00:00<00:00, 88.95it/s]\n",
            "[10 / 50] Train: Loss = 0.05065, Accuracy = 98.30%: 100%|██████████| 572/572 [00:04<00:00, 126.81it/s]\n",
            "[10 / 50]   Val: Loss = 0.19384, Accuracy = 93.56%: 100%|██████████| 13/13 [00:00<00:00, 102.76it/s]\n",
            "[11 / 50] Train: Loss = 0.04244, Accuracy = 98.58%: 100%|██████████| 572/572 [00:04<00:00, 125.27it/s]\n",
            "[11 / 50]   Val: Loss = 0.20440, Accuracy = 93.48%: 100%|██████████| 13/13 [00:00<00:00, 90.38it/s]\n",
            "[12 / 50] Train: Loss = 0.03610, Accuracy = 98.78%: 100%|██████████| 572/572 [00:04<00:00, 122.89it/s]\n",
            "[12 / 50]   Val: Loss = 0.21563, Accuracy = 93.47%: 100%|██████████| 13/13 [00:00<00:00, 96.37it/s]\n",
            "[13 / 50] Train: Loss = 0.03056, Accuracy = 98.97%: 100%|██████████| 572/572 [00:04<00:00, 125.45it/s]\n",
            "[13 / 50]   Val: Loss = 0.22276, Accuracy = 93.40%: 100%|██████████| 13/13 [00:00<00:00, 91.64it/s]\n",
            "[14 / 50] Train: Loss = 0.02664, Accuracy = 99.11%: 100%|██████████| 572/572 [00:04<00:00, 124.68it/s]\n",
            "[14 / 50]   Val: Loss = 0.22605, Accuracy = 93.43%: 100%|██████████| 13/13 [00:00<00:00, 91.45it/s]\n",
            "[15 / 50] Train: Loss = 0.02267, Accuracy = 99.23%: 100%|██████████| 572/572 [00:04<00:00, 127.39it/s]\n",
            "[15 / 50]   Val: Loss = 0.24529, Accuracy = 93.29%: 100%|██████████| 13/13 [00:00<00:00, 96.61it/s]\n",
            "[16 / 50] Train: Loss = 0.01989, Accuracy = 99.33%: 100%|██████████| 572/572 [00:04<00:00, 127.76it/s]\n",
            "[16 / 50]   Val: Loss = 0.24945, Accuracy = 93.36%: 100%|██████████| 13/13 [00:00<00:00, 93.08it/s]\n",
            "[17 / 50] Train: Loss = 0.01740, Accuracy = 99.41%: 100%|██████████| 572/572 [00:04<00:00, 131.31it/s]\n",
            "[17 / 50]   Val: Loss = 0.26541, Accuracy = 93.22%: 100%|██████████| 13/13 [00:00<00:00, 97.56it/s]\n",
            "[18 / 50] Train: Loss = 0.01635, Accuracy = 99.43%: 100%|██████████| 572/572 [00:04<00:00, 123.42it/s]\n",
            "[18 / 50]   Val: Loss = 0.25834, Accuracy = 93.27%: 100%|██████████| 13/13 [00:00<00:00, 97.31it/s]\n",
            "[19 / 50] Train: Loss = 0.01498, Accuracy = 99.47%: 100%|██████████| 572/572 [00:04<00:00, 123.24it/s]\n",
            "[19 / 50]   Val: Loss = 0.27089, Accuracy = 93.37%: 100%|██████████| 13/13 [00:00<00:00, 94.63it/s]\n",
            "[20 / 50] Train: Loss = 0.01334, Accuracy = 99.52%: 100%|██████████| 572/572 [00:04<00:00, 127.54it/s]\n",
            "[20 / 50]   Val: Loss = 0.27682, Accuracy = 93.29%: 100%|██████████| 13/13 [00:00<00:00, 100.41it/s]\n",
            "[21 / 50] Train: Loss = 0.01283, Accuracy = 99.52%: 100%|██████████| 572/572 [00:04<00:00, 130.80it/s]\n",
            "[21 / 50]   Val: Loss = 0.28512, Accuracy = 93.19%: 100%|██████████| 13/13 [00:00<00:00, 89.53it/s]\n",
            "[22 / 50] Train: Loss = 0.01265, Accuracy = 99.53%: 100%|██████████| 572/572 [00:04<00:00, 128.28it/s]\n",
            "[22 / 50]   Val: Loss = 0.29713, Accuracy = 93.25%: 100%|██████████| 13/13 [00:00<00:00, 99.65it/s] \n",
            "[23 / 50] Train: Loss = 0.01209, Accuracy = 99.55%: 100%|██████████| 572/572 [00:04<00:00, 129.70it/s]\n",
            "[23 / 50]   Val: Loss = 0.29776, Accuracy = 93.22%: 100%|██████████| 13/13 [00:00<00:00, 93.27it/s]\n",
            "[24 / 50] Train: Loss = 0.01189, Accuracy = 99.55%: 100%|██████████| 572/572 [00:04<00:00, 125.19it/s]\n",
            "[24 / 50]   Val: Loss = 0.30482, Accuracy = 93.10%: 100%|██████████| 13/13 [00:00<00:00, 99.54it/s] \n",
            "[25 / 50] Train: Loss = 0.01178, Accuracy = 99.54%: 100%|██████████| 572/572 [00:04<00:00, 124.30it/s]\n",
            "[25 / 50]   Val: Loss = 0.30762, Accuracy = 93.18%: 100%|██████████| 13/13 [00:00<00:00, 95.18it/s]\n",
            "[26 / 50] Train: Loss = 0.01058, Accuracy = 99.57%: 100%|██████████| 572/572 [00:04<00:00, 124.87it/s]\n",
            "[26 / 50]   Val: Loss = 0.31130, Accuracy = 93.20%: 100%|██████████| 13/13 [00:00<00:00, 92.33it/s]\n",
            "[27 / 50] Train: Loss = 0.01033, Accuracy = 99.58%: 100%|██████████| 572/572 [00:04<00:00, 128.78it/s]\n",
            "[27 / 50]   Val: Loss = 0.31767, Accuracy = 93.24%: 100%|██████████| 13/13 [00:00<00:00, 89.01it/s]\n",
            "[28 / 50] Train: Loss = 0.01053, Accuracy = 99.58%: 100%|██████████| 572/572 [00:04<00:00, 126.83it/s]\n",
            "[28 / 50]   Val: Loss = 0.31062, Accuracy = 93.29%: 100%|██████████| 13/13 [00:00<00:00, 96.79it/s]\n",
            "[29 / 50] Train: Loss = 0.01015, Accuracy = 99.59%: 100%|██████████| 572/572 [00:04<00:00, 127.47it/s]\n",
            "[29 / 50]   Val: Loss = 0.31248, Accuracy = 93.24%: 100%|██████████| 13/13 [00:00<00:00, 96.58it/s]\n",
            "[30 / 50] Train: Loss = 0.01004, Accuracy = 99.59%: 100%|██████████| 572/572 [00:04<00:00, 129.77it/s]\n",
            "[30 / 50]   Val: Loss = 0.32582, Accuracy = 93.11%: 100%|██████████| 13/13 [00:00<00:00, 95.76it/s]\n",
            "[31 / 50] Train: Loss = 0.01021, Accuracy = 99.58%: 100%|██████████| 572/572 [00:04<00:00, 123.22it/s]\n",
            "[31 / 50]   Val: Loss = 0.31738, Accuracy = 93.23%: 100%|██████████| 13/13 [00:00<00:00, 89.65it/s]\n",
            "[32 / 50] Train: Loss = 0.01048, Accuracy = 99.59%: 100%|██████████| 572/572 [00:04<00:00, 123.79it/s]\n",
            "[32 / 50]   Val: Loss = 0.33379, Accuracy = 93.18%: 100%|██████████| 13/13 [00:00<00:00, 92.40it/s]\n",
            "[33 / 50] Train: Loss = 0.01122, Accuracy = 99.57%: 100%|██████████| 572/572 [00:04<00:00, 126.41it/s]\n",
            "[33 / 50]   Val: Loss = 0.32312, Accuracy = 93.24%: 100%|██████████| 13/13 [00:00<00:00, 96.99it/s]\n",
            "[34 / 50] Train: Loss = 0.01082, Accuracy = 99.57%: 100%|██████████| 572/572 [00:04<00:00, 128.05it/s]\n",
            "[34 / 50]   Val: Loss = 0.33541, Accuracy = 93.20%: 100%|██████████| 13/13 [00:00<00:00, 98.95it/s]\n",
            "[35 / 50] Train: Loss = 0.00924, Accuracy = 99.62%: 100%|██████████| 572/572 [00:04<00:00, 125.56it/s]\n",
            "[35 / 50]   Val: Loss = 0.33549, Accuracy = 93.20%: 100%|██████████| 13/13 [00:00<00:00, 94.78it/s]\n",
            "[36 / 50] Train: Loss = 0.00918, Accuracy = 99.61%: 100%|██████████| 572/572 [00:04<00:00, 129.06it/s]\n",
            "[36 / 50]   Val: Loss = 0.33356, Accuracy = 93.24%: 100%|██████████| 13/13 [00:00<00:00, 96.25it/s]\n",
            "[37 / 50] Train: Loss = 0.00989, Accuracy = 99.60%: 100%|██████████| 572/572 [00:04<00:00, 125.33it/s]\n",
            "[37 / 50]   Val: Loss = 0.34092, Accuracy = 93.15%: 100%|██████████| 13/13 [00:00<00:00, 95.99it/s]\n",
            "[38 / 50] Train: Loss = 0.00944, Accuracy = 99.60%: 100%|██████████| 572/572 [00:04<00:00, 123.83it/s]\n",
            "[38 / 50]   Val: Loss = 0.34350, Accuracy = 93.21%: 100%|██████████| 13/13 [00:00<00:00, 90.68it/s]\n",
            "[39 / 50] Train: Loss = 0.00906, Accuracy = 99.61%: 100%|██████████| 572/572 [00:04<00:00, 125.03it/s]\n",
            "[39 / 50]   Val: Loss = 0.34356, Accuracy = 93.09%: 100%|██████████| 13/13 [00:00<00:00, 98.84it/s]\n",
            "[40 / 50] Train: Loss = 0.00960, Accuracy = 99.62%: 100%|██████████| 572/572 [00:04<00:00, 127.69it/s]\n",
            "[40 / 50]   Val: Loss = 0.34076, Accuracy = 93.24%: 100%|██████████| 13/13 [00:00<00:00, 98.93it/s]\n",
            "[41 / 50] Train: Loss = 0.01070, Accuracy = 99.58%: 100%|██████████| 572/572 [00:04<00:00, 128.29it/s]\n",
            "[41 / 50]   Val: Loss = 0.35171, Accuracy = 93.12%: 100%|██████████| 13/13 [00:00<00:00, 100.05it/s]\n",
            "[42 / 50] Train: Loss = 0.01013, Accuracy = 99.57%: 100%|██████████| 572/572 [00:04<00:00, 128.31it/s]\n",
            "[42 / 50]   Val: Loss = 0.34652, Accuracy = 93.16%: 100%|██████████| 13/13 [00:00<00:00, 103.22it/s]\n",
            "[43 / 50] Train: Loss = 0.00872, Accuracy = 99.61%: 100%|██████████| 572/572 [00:04<00:00, 127.24it/s]\n",
            "[43 / 50]   Val: Loss = 0.34282, Accuracy = 93.14%: 100%|██████████| 13/13 [00:00<00:00, 94.99it/s]\n",
            "[44 / 50] Train: Loss = 0.00865, Accuracy = 99.62%: 100%|██████████| 572/572 [00:04<00:00, 123.99it/s]\n",
            "[44 / 50]   Val: Loss = 0.35276, Accuracy = 93.14%: 100%|██████████| 13/13 [00:00<00:00, 95.90it/s]\n",
            "[45 / 50] Train: Loss = 0.00909, Accuracy = 99.62%: 100%|██████████| 572/572 [00:04<00:00, 124.64it/s]\n",
            "[45 / 50]   Val: Loss = 0.35733, Accuracy = 93.15%: 100%|██████████| 13/13 [00:00<00:00, 93.10it/s]\n",
            "[46 / 50] Train: Loss = 0.01026, Accuracy = 99.58%: 100%|██████████| 572/572 [00:04<00:00, 124.27it/s]\n",
            "[46 / 50]   Val: Loss = 0.35675, Accuracy = 93.15%: 100%|██████████| 13/13 [00:00<00:00, 97.49it/s]\n",
            "[47 / 50] Train: Loss = 0.00991, Accuracy = 99.60%: 100%|██████████| 572/572 [00:04<00:00, 128.37it/s]\n",
            "[47 / 50]   Val: Loss = 0.35231, Accuracy = 93.14%: 100%|██████████| 13/13 [00:00<00:00, 95.23it/s]\n",
            "[48 / 50] Train: Loss = 0.00914, Accuracy = 99.62%: 100%|██████████| 572/572 [00:04<00:00, 125.86it/s]\n",
            "[48 / 50]   Val: Loss = 0.35976, Accuracy = 93.11%: 100%|██████████| 13/13 [00:00<00:00, 97.03it/s]\n",
            "[49 / 50] Train: Loss = 0.00882, Accuracy = 99.61%: 100%|██████████| 572/572 [00:04<00:00, 127.60it/s]\n",
            "[49 / 50]   Val: Loss = 0.35673, Accuracy = 93.11%: 100%|██████████| 13/13 [00:00<00:00, 99.19it/s] \n",
            "[50 / 50] Train: Loss = 0.00866, Accuracy = 99.62%: 100%|██████████| 572/572 [00:04<00:00, 122.98it/s]\n",
            "[50 / 50]   Val: Loss = 0.37712, Accuracy = 93.12%: 100%|██████████| 13/13 [00:00<00:00, 94.48it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "98wr38_rw55D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "93c2fd23-ae30-4d81-bcc3-75148e118e49"
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_test, y_test), len(X_test)))\n",
        "X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "truth, n = calc_accuracy(model(X_batch), y_batch)\n",
        "truth / n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9322, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqiPCODS0cHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.lstm_layers_count = lstm_layers_count\n",
        "        self.embed = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
        "        self.fc = nn.Linear(2 * lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embed(inputs)\n",
        "        before_tags, _ = self.lstm(embeds)\n",
        "        tags = self.fc(before_tags)\n",
        "        softmax_tags = F.log_softmax(tags, dim=1)\n",
        "        return softmax_tags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "112UXCYQqa9J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ccf90dbf-79fa-4609-9e1e-c6796f99d0d6"
      },
      "source": [
        "model = BiLSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.59883, Accuracy = 81.75%: 100%|██████████| 572/572 [00:05<00:00, 100.78it/s]\n",
            "[1 / 50]   Val: Loss = 0.31206, Accuracy = 89.92%: 100%|██████████| 13/13 [00:00<00:00, 76.08it/s]\n",
            "[2 / 50] Train: Loss = 0.23143, Accuracy = 92.73%: 100%|██████████| 572/572 [00:05<00:00, 100.02it/s]\n",
            "[2 / 50]   Val: Loss = 0.21148, Accuracy = 93.05%: 100%|██████████| 13/13 [00:00<00:00, 74.96it/s]\n",
            "[3 / 50] Train: Loss = 0.15183, Accuracy = 95.33%: 100%|██████████| 572/572 [00:05<00:00, 102.56it/s]\n",
            "[3 / 50]   Val: Loss = 0.16694, Accuracy = 94.49%: 100%|██████████| 13/13 [00:00<00:00, 77.75it/s]\n",
            "[4 / 50] Train: Loss = 0.10823, Accuracy = 96.71%: 100%|██████████| 572/572 [00:05<00:00, 103.39it/s]\n",
            "[4 / 50]   Val: Loss = 0.15206, Accuracy = 94.97%: 100%|██████████| 13/13 [00:00<00:00, 77.15it/s]\n",
            "[5 / 50] Train: Loss = 0.08021, Accuracy = 97.58%: 100%|██████████| 572/572 [00:05<00:00, 103.28it/s]\n",
            "[5 / 50]   Val: Loss = 0.14358, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 77.56it/s]\n",
            "[6 / 50] Train: Loss = 0.05934, Accuracy = 98.24%: 100%|██████████| 572/572 [00:05<00:00, 102.69it/s]\n",
            "[6 / 50]   Val: Loss = 0.13586, Accuracy = 95.49%: 100%|██████████| 13/13 [00:00<00:00, 78.73it/s]\n",
            "[7 / 50] Train: Loss = 0.04427, Accuracy = 98.71%: 100%|██████████| 572/572 [00:05<00:00, 101.02it/s]\n",
            "[7 / 50]   Val: Loss = 0.13992, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 75.56it/s]\n",
            "[8 / 50] Train: Loss = 0.03296, Accuracy = 99.05%: 100%|██████████| 572/572 [00:05<00:00, 103.41it/s]\n",
            "[8 / 50]   Val: Loss = 0.13851, Accuracy = 95.60%: 100%|██████████| 13/13 [00:00<00:00, 75.26it/s]\n",
            "[9 / 50] Train: Loss = 0.02392, Accuracy = 99.32%: 100%|██████████| 572/572 [00:05<00:00, 102.57it/s]\n",
            "[9 / 50]   Val: Loss = 0.14160, Accuracy = 95.63%: 100%|██████████| 13/13 [00:00<00:00, 75.08it/s]\n",
            "[10 / 50] Train: Loss = 0.01879, Accuracy = 99.48%: 100%|██████████| 572/572 [00:05<00:00, 101.72it/s]\n",
            "[10 / 50]   Val: Loss = 0.15502, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 75.32it/s]\n",
            "[11 / 50] Train: Loss = 0.01525, Accuracy = 99.56%: 100%|██████████| 572/572 [00:05<00:00, 99.51it/s]\n",
            "[11 / 50]   Val: Loss = 0.16166, Accuracy = 95.40%: 100%|██████████| 13/13 [00:00<00:00, 75.16it/s]\n",
            "[12 / 50] Train: Loss = 0.01186, Accuracy = 99.66%: 100%|██████████| 572/572 [00:05<00:00, 100.83it/s]\n",
            "[12 / 50]   Val: Loss = 0.17100, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 79.75it/s]\n",
            "[13 / 50] Train: Loss = 0.01018, Accuracy = 99.70%: 100%|██████████| 572/572 [00:05<00:00, 101.26it/s]\n",
            "[13 / 50]   Val: Loss = 0.18024, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 75.01it/s]\n",
            "[14 / 50] Train: Loss = 0.00988, Accuracy = 99.71%: 100%|██████████| 572/572 [00:05<00:00, 102.94it/s]\n",
            "[14 / 50]   Val: Loss = 0.17619, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 77.73it/s]\n",
            "[15 / 50] Train: Loss = 0.00874, Accuracy = 99.73%: 100%|██████████| 572/572 [00:05<00:00, 103.00it/s]\n",
            "[15 / 50]   Val: Loss = 0.18724, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 75.68it/s]\n",
            "[16 / 50] Train: Loss = 0.00740, Accuracy = 99.77%: 100%|██████████| 572/572 [00:05<00:00, 101.31it/s]\n",
            "[16 / 50]   Val: Loss = 0.18536, Accuracy = 95.41%: 100%|██████████| 13/13 [00:00<00:00, 72.18it/s]\n",
            "[17 / 50] Train: Loss = 0.00782, Accuracy = 99.74%: 100%|██████████| 572/572 [00:05<00:00, 100.56it/s]\n",
            "[17 / 50]   Val: Loss = 0.19800, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 75.12it/s]\n",
            "[18 / 50] Train: Loss = 0.00680, Accuracy = 99.79%: 100%|██████████| 572/572 [00:05<00:00, 103.53it/s]\n",
            "[18 / 50]   Val: Loss = 0.20581, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 73.66it/s]\n",
            "[19 / 50] Train: Loss = 0.00713, Accuracy = 99.76%: 100%|██████████| 572/572 [00:05<00:00, 102.83it/s]\n",
            "[19 / 50]   Val: Loss = 0.21512, Accuracy = 95.21%: 100%|██████████| 13/13 [00:00<00:00, 73.84it/s]\n",
            "[20 / 50] Train: Loss = 0.00608, Accuracy = 99.79%: 100%|██████████| 572/572 [00:05<00:00, 103.37it/s]\n",
            "[20 / 50]   Val: Loss = 0.20637, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 77.97it/s]\n",
            "[21 / 50] Train: Loss = 0.00600, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 101.69it/s]\n",
            "[21 / 50]   Val: Loss = 0.20955, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 77.83it/s]\n",
            "[22 / 50] Train: Loss = 0.00746, Accuracy = 99.76%: 100%|██████████| 572/572 [00:05<00:00, 100.11it/s]\n",
            "[22 / 50]   Val: Loss = 0.22149, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 76.44it/s]\n",
            "[23 / 50] Train: Loss = 0.00609, Accuracy = 99.78%: 100%|██████████| 572/572 [00:05<00:00, 102.98it/s]\n",
            "[23 / 50]   Val: Loss = 0.22605, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 74.86it/s]\n",
            "[24 / 50] Train: Loss = 0.00531, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 104.65it/s]\n",
            "[24 / 50]   Val: Loss = 0.22880, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 77.38it/s]\n",
            "[25 / 50] Train: Loss = 0.00488, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 102.50it/s]\n",
            "[25 / 50]   Val: Loss = 0.23724, Accuracy = 95.18%: 100%|██████████| 13/13 [00:00<00:00, 76.46it/s]\n",
            "[26 / 50] Train: Loss = 0.00539, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 102.63it/s]\n",
            "[26 / 50]   Val: Loss = 0.23078, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 77.53it/s]\n",
            "[27 / 50] Train: Loss = 0.00586, Accuracy = 99.79%: 100%|██████████| 572/572 [00:05<00:00, 101.10it/s]\n",
            "[27 / 50]   Val: Loss = 0.24108, Accuracy = 95.24%: 100%|██████████| 13/13 [00:00<00:00, 77.09it/s]\n",
            "[28 / 50] Train: Loss = 0.00501, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 101.66it/s]\n",
            "[28 / 50]   Val: Loss = 0.24870, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 77.08it/s]\n",
            "[29 / 50] Train: Loss = 0.00459, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 103.66it/s]\n",
            "[29 / 50]   Val: Loss = 0.24518, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 70.91it/s]\n",
            "[30 / 50] Train: Loss = 0.00459, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 102.91it/s]\n",
            "[30 / 50]   Val: Loss = 0.25721, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 76.03it/s]\n",
            "[31 / 50] Train: Loss = 0.00468, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 100.96it/s]\n",
            "[31 / 50]   Val: Loss = 0.23345, Accuracy = 95.40%: 100%|██████████| 13/13 [00:00<00:00, 73.04it/s]\n",
            "[32 / 50] Train: Loss = 0.00467, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 101.28it/s]\n",
            "[32 / 50]   Val: Loss = 0.25102, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 73.31it/s]\n",
            "[33 / 50] Train: Loss = 0.00494, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 99.44it/s] \n",
            "[33 / 50]   Val: Loss = 0.25082, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 75.23it/s]\n",
            "[34 / 50] Train: Loss = 0.00495, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 100.77it/s]\n",
            "[34 / 50]   Val: Loss = 0.27215, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 78.13it/s]\n",
            "[35 / 50] Train: Loss = 0.00467, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 101.50it/s]\n",
            "[35 / 50]   Val: Loss = 0.25849, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 73.95it/s]\n",
            "[36 / 50] Train: Loss = 0.00443, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 102.57it/s]\n",
            "[36 / 50]   Val: Loss = 0.24711, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 72.60it/s]\n",
            "[37 / 50] Train: Loss = 0.00468, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 100.89it/s]\n",
            "[37 / 50]   Val: Loss = 0.26275, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 74.53it/s]\n",
            "[38 / 50] Train: Loss = 0.00524, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 100.98it/s]\n",
            "[38 / 50]   Val: Loss = 0.25287, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 77.18it/s]\n",
            "[39 / 50] Train: Loss = 0.00493, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 102.70it/s]\n",
            "[39 / 50]   Val: Loss = 0.25659, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 77.99it/s]\n",
            "[40 / 50] Train: Loss = 0.00446, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 102.40it/s]\n",
            "[40 / 50]   Val: Loss = 0.26050, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 77.51it/s]\n",
            "[41 / 50] Train: Loss = 0.00471, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 101.09it/s]\n",
            "[41 / 50]   Val: Loss = 0.25906, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 72.99it/s]\n",
            "[42 / 50] Train: Loss = 0.00522, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 100.91it/s]\n",
            "[42 / 50]   Val: Loss = 0.25185, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 77.37it/s]\n",
            "[43 / 50] Train: Loss = 0.00509, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 101.32it/s]\n",
            "[43 / 50]   Val: Loss = 0.25959, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 73.13it/s]\n",
            "[44 / 50] Train: Loss = 0.00427, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 101.63it/s]\n",
            "[44 / 50]   Val: Loss = 0.26582, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 77.19it/s]\n",
            "[45 / 50] Train: Loss = 0.00419, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 101.18it/s]\n",
            "[45 / 50]   Val: Loss = 0.26783, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 80.34it/s]\n",
            "[46 / 50] Train: Loss = 0.00412, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 103.24it/s]\n",
            "[46 / 50]   Val: Loss = 0.27242, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 76.50it/s]\n",
            "[47 / 50] Train: Loss = 0.00468, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 99.62it/s]\n",
            "[47 / 50]   Val: Loss = 0.27504, Accuracy = 95.34%: 100%|██████████| 13/13 [00:00<00:00, 79.08it/s]\n",
            "[48 / 50] Train: Loss = 0.00565, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 100.40it/s]\n",
            "[48 / 50]   Val: Loss = 0.26547, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 74.31it/s]\n",
            "[49 / 50] Train: Loss = 0.00473, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 101.06it/s]\n",
            "[49 / 50]   Val: Loss = 0.26328, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 77.34it/s]\n",
            "[50 / 50] Train: Loss = 0.00409, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 102.02it/s]\n",
            "[50 / 50]   Val: Loss = 0.27376, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 77.42it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4W5YxFKqdnk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "feca2704-8d6f-4813-d3b2-6b3c58a4d485"
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_test, y_test), len(X_test)))\n",
        "X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "truth, n = calc_accuracy(model(X_batch), y_batch)\n",
        "truth / n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9538, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uZpY_Q1xZ18h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "2f16254c-75e5-445e-aec5-b088ada8ac8c"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VsCstxiO03oT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "19caf1dd-b04a-4149-f5f3-ff377c943caf"
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LxaRBpQd0pat",
        "colab": {}
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.lstm_layers_count = lstm_layers_count\n",
        "        self.embed = nn.Embedding.from_pretrained(embeddings)\n",
        "        self.lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
        "        self.fc = nn.Linear(2 * lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embed(inputs)\n",
        "        before_tags, _ = self.lstm(embeds)\n",
        "        tags = self.fc(before_tags)\n",
        "        softmax_tags = F.log_softmax(tags, dim=1)\n",
        "        return softmax_tags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBtI6BDE-Fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "72ed38fa-b0b9-4d84-8d82-2e45b071e2b2"
      },
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=FloatTensor(embeddings),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.60346, Accuracy = 84.12%: 100%|██████████| 572/572 [00:05<00:00, 111.17it/s]\n",
            "[1 / 50]   Val: Loss = 0.25991, Accuracy = 92.29%: 100%|██████████| 13/13 [00:00<00:00, 94.00it/s]\n",
            "[2 / 50] Train: Loss = 0.20837, Accuracy = 93.83%: 100%|██████████| 572/572 [00:05<00:00, 113.90it/s]\n",
            "[2 / 50]   Val: Loss = 0.17944, Accuracy = 94.65%: 100%|██████████| 13/13 [00:00<00:00, 96.75it/s]\n",
            "[3 / 50] Train: Loss = 0.15319, Accuracy = 95.42%: 100%|██████████| 572/572 [00:05<00:00, 113.08it/s]\n",
            "[3 / 50]   Val: Loss = 0.14785, Accuracy = 95.48%: 100%|██████████| 13/13 [00:00<00:00, 97.82it/s]\n",
            "[4 / 50] Train: Loss = 0.12632, Accuracy = 96.16%: 100%|██████████| 572/572 [00:05<00:00, 112.65it/s]\n",
            "[4 / 50]   Val: Loss = 0.13019, Accuracy = 96.00%: 100%|██████████| 13/13 [00:00<00:00, 99.03it/s]\n",
            "[5 / 50] Train: Loss = 0.10872, Accuracy = 96.67%: 100%|██████████| 572/572 [00:05<00:00, 110.07it/s]\n",
            "[5 / 50]   Val: Loss = 0.12117, Accuracy = 96.22%: 100%|██████████| 13/13 [00:00<00:00, 96.84it/s]\n",
            "[6 / 50] Train: Loss = 0.09797, Accuracy = 96.99%: 100%|██████████| 572/572 [00:05<00:00, 111.96it/s]\n",
            "[6 / 50]   Val: Loss = 0.11448, Accuracy = 96.46%: 100%|██████████| 13/13 [00:00<00:00, 101.56it/s]\n",
            "[7 / 50] Train: Loss = 0.09032, Accuracy = 97.20%: 100%|██████████| 572/572 [00:04<00:00, 114.61it/s]\n",
            "[7 / 50]   Val: Loss = 0.10873, Accuracy = 96.60%: 100%|██████████| 13/13 [00:00<00:00, 99.83it/s]\n",
            "[8 / 50] Train: Loss = 0.08382, Accuracy = 97.39%: 100%|██████████| 572/572 [00:05<00:00, 113.38it/s]\n",
            "[8 / 50]   Val: Loss = 0.10671, Accuracy = 96.60%: 100%|██████████| 13/13 [00:00<00:00, 98.71it/s]\n",
            "[9 / 50] Train: Loss = 0.07874, Accuracy = 97.53%: 100%|██████████| 572/572 [00:04<00:00, 115.30it/s]\n",
            "[9 / 50]   Val: Loss = 0.10481, Accuracy = 96.75%: 100%|██████████| 13/13 [00:00<00:00, 95.08it/s]\n",
            "[10 / 50] Train: Loss = 0.07411, Accuracy = 97.67%: 100%|██████████| 572/572 [00:05<00:00, 113.34it/s]\n",
            "[10 / 50]   Val: Loss = 0.10071, Accuracy = 96.83%: 100%|██████████| 13/13 [00:00<00:00, 90.94it/s]\n",
            "[11 / 50] Train: Loss = 0.07047, Accuracy = 97.78%: 100%|██████████| 572/572 [00:05<00:00, 114.39it/s]\n",
            "[11 / 50]   Val: Loss = 0.09898, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 88.93it/s]\n",
            "[12 / 50] Train: Loss = 0.06671, Accuracy = 97.88%: 100%|██████████| 572/572 [00:05<00:00, 113.78it/s]\n",
            "[12 / 50]   Val: Loss = 0.09835, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 94.65it/s]\n",
            "[13 / 50] Train: Loss = 0.06384, Accuracy = 97.95%: 100%|██████████| 572/572 [00:04<00:00, 114.43it/s]\n",
            "[13 / 50]   Val: Loss = 0.09787, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 93.96it/s]\n",
            "[14 / 50] Train: Loss = 0.06166, Accuracy = 98.04%: 100%|██████████| 572/572 [00:04<00:00, 117.30it/s]\n",
            "[14 / 50]   Val: Loss = 0.09636, Accuracy = 96.95%: 100%|██████████| 13/13 [00:00<00:00, 97.86it/s]\n",
            "[15 / 50] Train: Loss = 0.05938, Accuracy = 98.09%: 100%|██████████| 572/572 [00:04<00:00, 116.11it/s]\n",
            "[15 / 50]   Val: Loss = 0.09572, Accuracy = 96.96%: 100%|██████████| 13/13 [00:00<00:00, 101.22it/s]\n",
            "[16 / 50] Train: Loss = 0.05675, Accuracy = 98.18%: 100%|██████████| 572/572 [00:05<00:00, 111.14it/s]\n",
            "[16 / 50]   Val: Loss = 0.09472, Accuracy = 97.00%: 100%|██████████| 13/13 [00:00<00:00, 90.86it/s]\n",
            "[17 / 50] Train: Loss = 0.05515, Accuracy = 98.22%: 100%|██████████| 572/572 [00:05<00:00, 114.27it/s]\n",
            "[17 / 50]   Val: Loss = 0.09586, Accuracy = 96.95%: 100%|██████████| 13/13 [00:00<00:00, 93.91it/s]\n",
            "[18 / 50] Train: Loss = 0.05294, Accuracy = 98.28%: 100%|██████████| 572/572 [00:04<00:00, 115.55it/s]\n",
            "[18 / 50]   Val: Loss = 0.09571, Accuracy = 96.99%: 100%|██████████| 13/13 [00:00<00:00, 94.90it/s]\n",
            "[19 / 50] Train: Loss = 0.05105, Accuracy = 98.34%: 100%|██████████| 572/572 [00:04<00:00, 115.81it/s]\n",
            "[19 / 50]   Val: Loss = 0.09434, Accuracy = 96.99%: 100%|██████████| 13/13 [00:00<00:00, 98.04it/s]\n",
            "[20 / 50] Train: Loss = 0.04954, Accuracy = 98.39%: 100%|██████████| 572/572 [00:05<00:00, 113.88it/s]\n",
            "[20 / 50]   Val: Loss = 0.09525, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 95.93it/s]\n",
            "[21 / 50] Train: Loss = 0.04752, Accuracy = 98.45%: 100%|██████████| 572/572 [00:05<00:00, 111.70it/s]\n",
            "[21 / 50]   Val: Loss = 0.09665, Accuracy = 96.93%: 100%|██████████| 13/13 [00:00<00:00, 91.43it/s]\n",
            "[22 / 50] Train: Loss = 0.04633, Accuracy = 98.49%: 100%|██████████| 572/572 [00:05<00:00, 112.77it/s]\n",
            "[22 / 50]   Val: Loss = 0.09868, Accuracy = 96.96%: 100%|██████████| 13/13 [00:00<00:00, 96.97it/s]\n",
            "[23 / 50] Train: Loss = 0.04502, Accuracy = 98.54%: 100%|██████████| 572/572 [00:05<00:00, 113.45it/s]\n",
            "[23 / 50]   Val: Loss = 0.09839, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 92.29it/s]\n",
            "[24 / 50] Train: Loss = 0.04381, Accuracy = 98.59%: 100%|██████████| 572/572 [00:05<00:00, 112.89it/s]\n",
            "[24 / 50]   Val: Loss = 0.09890, Accuracy = 96.98%: 100%|██████████| 13/13 [00:00<00:00, 94.93it/s]\n",
            "[25 / 50] Train: Loss = 0.04216, Accuracy = 98.64%: 100%|██████████| 572/572 [00:05<00:00, 113.49it/s]\n",
            "[25 / 50]   Val: Loss = 0.09955, Accuracy = 96.96%: 100%|██████████| 13/13 [00:00<00:00, 100.86it/s]\n",
            "[26 / 50] Train: Loss = 0.04072, Accuracy = 98.67%: 100%|██████████| 572/572 [00:05<00:00, 111.66it/s]\n",
            "[26 / 50]   Val: Loss = 0.10000, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 100.56it/s]\n",
            "[27 / 50] Train: Loss = 0.03982, Accuracy = 98.72%: 100%|██████████| 572/572 [00:04<00:00, 115.16it/s]\n",
            "[27 / 50]   Val: Loss = 0.10160, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 92.21it/s]\n",
            "[28 / 50] Train: Loss = 0.03868, Accuracy = 98.75%: 100%|██████████| 572/572 [00:04<00:00, 115.23it/s]\n",
            "[28 / 50]   Val: Loss = 0.10399, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 96.51it/s]\n",
            "[29 / 50] Train: Loss = 0.03821, Accuracy = 98.79%: 100%|██████████| 572/572 [00:05<00:00, 113.37it/s]\n",
            "[29 / 50]   Val: Loss = 0.10225, Accuracy = 96.95%: 100%|██████████| 13/13 [00:00<00:00, 100.52it/s]\n",
            "[30 / 50] Train: Loss = 0.03632, Accuracy = 98.83%: 100%|██████████| 572/572 [00:04<00:00, 115.69it/s]\n",
            "[30 / 50]   Val: Loss = 0.10679, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 98.16it/s]\n",
            "[31 / 50] Train: Loss = 0.03596, Accuracy = 98.85%: 100%|██████████| 572/572 [00:04<00:00, 114.42it/s]\n",
            "[31 / 50]   Val: Loss = 0.10616, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 100.65it/s]\n",
            "[32 / 50] Train: Loss = 0.03486, Accuracy = 98.90%: 100%|██████████| 572/572 [00:04<00:00, 115.59it/s]\n",
            "[32 / 50]   Val: Loss = 0.10603, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 100.02it/s]\n",
            "[33 / 50] Train: Loss = 0.03398, Accuracy = 98.92%: 100%|██████████| 572/572 [00:05<00:00, 112.59it/s]\n",
            "[33 / 50]   Val: Loss = 0.10672, Accuracy = 96.85%: 100%|██████████| 13/13 [00:00<00:00, 95.23it/s]\n",
            "[34 / 50] Train: Loss = 0.03292, Accuracy = 98.98%: 100%|██████████| 572/572 [00:05<00:00, 111.16it/s]\n",
            "[34 / 50]   Val: Loss = 0.10865, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 92.08it/s]\n",
            "[35 / 50] Train: Loss = 0.03282, Accuracy = 98.96%: 100%|██████████| 572/572 [00:05<00:00, 110.33it/s]\n",
            "[35 / 50]   Val: Loss = 0.10905, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 93.64it/s]\n",
            "[36 / 50] Train: Loss = 0.03108, Accuracy = 99.00%: 100%|██████████| 572/572 [00:05<00:00, 111.71it/s]\n",
            "[36 / 50]   Val: Loss = 0.11001, Accuracy = 96.84%: 100%|██████████| 13/13 [00:00<00:00, 99.47it/s]\n",
            "[37 / 50] Train: Loss = 0.02993, Accuracy = 99.08%: 100%|██████████| 572/572 [00:04<00:00, 115.70it/s]\n",
            "[37 / 50]   Val: Loss = 0.11172, Accuracy = 96.83%: 100%|██████████| 13/13 [00:00<00:00, 92.69it/s]\n",
            "[38 / 50] Train: Loss = 0.02956, Accuracy = 99.07%: 100%|██████████| 572/572 [00:04<00:00, 115.63it/s]\n",
            "[38 / 50]   Val: Loss = 0.11359, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 91.47it/s]\n",
            "[39 / 50] Train: Loss = 0.02871, Accuracy = 99.11%: 100%|██████████| 572/572 [00:05<00:00, 111.51it/s]\n",
            "[39 / 50]   Val: Loss = 0.11539, Accuracy = 96.76%: 100%|██████████| 13/13 [00:00<00:00, 95.27it/s]\n",
            "[40 / 50] Train: Loss = 0.02843, Accuracy = 99.11%: 100%|██████████| 572/572 [00:05<00:00, 109.77it/s]\n",
            "[40 / 50]   Val: Loss = 0.11509, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 91.53it/s]\n",
            "[41 / 50] Train: Loss = 0.02761, Accuracy = 99.13%: 100%|██████████| 572/572 [00:05<00:00, 110.56it/s]\n",
            "[41 / 50]   Val: Loss = 0.11888, Accuracy = 96.82%: 100%|██████████| 13/13 [00:00<00:00, 95.93it/s]\n",
            "[42 / 50] Train: Loss = 0.02709, Accuracy = 99.16%: 100%|██████████| 572/572 [00:05<00:00, 113.78it/s]\n",
            "[42 / 50]   Val: Loss = 0.11853, Accuracy = 96.73%: 100%|██████████| 13/13 [00:00<00:00, 96.79it/s]\n",
            "[43 / 50] Train: Loss = 0.02651, Accuracy = 99.17%: 100%|██████████| 572/572 [00:05<00:00, 112.72it/s]\n",
            "[43 / 50]   Val: Loss = 0.11879, Accuracy = 96.76%: 100%|██████████| 13/13 [00:00<00:00, 99.33it/s]\n",
            "[44 / 50] Train: Loss = 0.02528, Accuracy = 99.21%: 100%|██████████| 572/572 [00:05<00:00, 113.61it/s]\n",
            "[44 / 50]   Val: Loss = 0.11930, Accuracy = 96.75%: 100%|██████████| 13/13 [00:00<00:00, 89.83it/s]\n",
            "[45 / 50] Train: Loss = 0.02485, Accuracy = 99.23%: 100%|██████████| 572/572 [00:05<00:00, 113.04it/s]\n",
            "[45 / 50]   Val: Loss = 0.12082, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 95.67it/s] \n",
            "[46 / 50] Train: Loss = 0.02430, Accuracy = 99.26%: 100%|██████████| 572/572 [00:05<00:00, 113.17it/s]\n",
            "[46 / 50]   Val: Loss = 0.12547, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 95.86it/s]\n",
            "[47 / 50] Train: Loss = 0.02390, Accuracy = 99.26%: 100%|██████████| 572/572 [00:04<00:00, 114.62it/s]\n",
            "[47 / 50]   Val: Loss = 0.12543, Accuracy = 96.73%: 100%|██████████| 13/13 [00:00<00:00, 93.99it/s]\n",
            "[48 / 50] Train: Loss = 0.02313, Accuracy = 99.28%: 100%|██████████| 572/572 [00:05<00:00, 114.29it/s]\n",
            "[48 / 50]   Val: Loss = 0.12941, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 91.33it/s]\n",
            "[49 / 50] Train: Loss = 0.02190, Accuracy = 99.33%: 100%|██████████| 572/572 [00:04<00:00, 117.20it/s]\n",
            "[49 / 50]   Val: Loss = 0.13019, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 98.15it/s]\n",
            "[50 / 50] Train: Loss = 0.02157, Accuracy = 99.33%: 100%|██████████| 572/572 [00:04<00:00, 116.57it/s]\n",
            "[50 / 50]   Val: Loss = 0.13232, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 92.80it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HPUuAPGhEGVR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6935223a-3c02-43b2-beed-74953ee3c8c0"
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_test, y_test), len(X_test)))\n",
        "X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "truth, n = calc_accuracy(model(X_batch), y_batch)\n",
        "truth / n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9679, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    }
  ]
}